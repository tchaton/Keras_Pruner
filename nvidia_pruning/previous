import os
import types
from keras import optimizers
import inspect
import numpy as np
import keras.backend as K
from keras.optimizers import TFOptimizer
import glob
from collections import defaultdict
from sklearn.metrics import accuracy_score, f1_score
from keras.models import clone_model, Model
import collections
import random
from deap import  base, creator, tools
import gc
from nn.optimizers import  YFOptimizer, FTML
from nn_test.data import generator
from utils.utils_exceptions import InputError
from utils.utils import mk
import tensorflow as tf
from config import ROOT_PRUNED

def join_path(list_path):
	path = '~'
	for p in list_path:
		path = os.path.join(path, p)
	return path

def get_class_params(_class):
	return inspect.getargspec (_class.__init__)

def filter_args(_class, args):
	_class_args = get_class_params(_class).args
	print(_class_args)
	d = {}
	for k in _class_args[1:]:
		try:
			d[k] = args[k]
		except:
			pass
	return d




class NVIDIA_Pruner:

	def __init__(self, model, name, optim, nb_class=None, with_debug=True, mode='kernel_only'):
		gc.collect()
		self.model = clone_model(model)
		self.name = name
		self.optim = optim
		self.path = os.path.join(ROOT_PRUNED, name)
		if isinstance(nb_class, types.NoneType):
			raise Exception
		self.nb_class = nb_class
		self.mode = mode
		self.get_paths()
		self.names = self.get_names()(model)
		self.with_debug = with_debug
		self.eval_model = clone_model(model)

	def filter_tensor_name(self, w_name):
		return str('_'.join(w_name.split('/')[0].split('_')[:2]))

	def load_test(self, pipeline, generator_func, model_type, print_value=200):
		self.model_type = model_type
		self.L = len(pipeline.augmentor_images)
		n = np.zeros((self.L,))
		self.X = []
		self.Y = []
		cnt = 0
		for X, Y, Z in generator_func(pipeline, self.model_type, with_indexes=True):
			if cnt > self.L:
				break
			for x, y, z in zip(X, Y, Z):
				if n[z] == 0:
					n[z] = 1
					self.X.append(x)
					self.Y.append(y)
				cnt += 1
			if cnt % print_value == 0:
				print(cnt)
		self.X = np.array(self.X)
		self.Y = np.array(self.Y)

	def convert(self, text, reverse=True, remove_end=True):
		if reverse:
			splits = text.split('_')
			text = os.path.join('_'.join(splits[:-1]), splits[-1])
			if remove_end:
				return text.split('/')[0]
			else:
				return text
		else:
			return text.replace('/','_')

	def get_weight_names(self):
		self.w_names = list(set([str(self.filter_tensor_name(w_name.name)) for w_name in self.model.trainable_weights]))
		print(self.w_names)

	def get_paths(self):
		self.get_weight_names()
		self.paths = []
		for w_name in self.w_names:
			path = join_path([ROOT_PRUNED, self.name, 'scores', w_name, '*', self.optim + '.npy'])
			self.paths.append(glob.glob(path))

	def init_pruning(self):
		self.load_scores()

	def get_layer_shape(self, name):
		return self.model.get_layer(name).output.eval(session=K.get_session(),feed_dict={
			self.model.input: np.random.normal(0, 1, (1, 224, 224, 3))
		})

	def load_scores(self, normalize_mode=0):
		d = defaultdict(list)
		L = len(ROOT_PRUNED.split('/'))
		for path_weights in self.paths:
			w_name, _, _ =  (path_weights[0].split('/')[L+2:])
			for path in path_weights:
				if self.mode == 'kernel_only':
					d[w_name].append(np.load(path))

		self.dict_shape = {}

		if self.mode == 'kernel_only' or self.mode == 'kernel_and_biais':
			for key in d.keys():
				values, normas = np.array([e[0] for e in d[key]]), np.array([e[1] for e in d[key]])
				shape = self.get_layer_shape(key).shape
				self.dict_shape[key] = shape[1:]
				p = np.product(shape[1:])
				if normalize_mode == 0:
					h = [v/(n*p) for v, n in zip(values, normas)]
				elif normalize_mode == 1:
					h = [v/float(p) for v, _ in zip(values, normas)]
				elif normalize_mode == 2:
					h = values
				mean = np.mean(h, axis=0)
				d[key] = mean#/np.linalg.norm(mean)
		self.scores = d

	def create_subset(self, percentage):
		L = len(self.X)
		mask = np.random.permutation(range(L))
		indexes = np.where(mask < int(percentage * L))
		print(len(indexes[0]))
		self.indexes = indexes

	def evaluate(self, model, frozen, is_returned=True, percentage=0.02):
		if not frozen:
			self.create_subset(percentage)
		preds = model.predict(self.X[self.indexes])
		y_true, y_pred = np.argmax(self.Y[self.indexes], axis=1), np.argmax(preds, axis=1)
		score = accuracy_score(y_true, y_pred)
		print(f1_score(y_true, y_pred, average='macro'))
		if not frozen:
			print(score)
			self.current_acc = score
		if is_returned:
			return score

	def get_layer(self, model, layer_name):
		return get_layer_rec(model, layer_name)

	def get_names(self):
		from keras.models import Model
		holder_names = []
		def func(model):
			for l in model.layers:
				if isinstance(l, Model):
					func(l)
				else:
					holder_names.append(l.name)
			return list(set(holder_names))

		return func

	def prune_layer_per_percentage(self, layer, key, q_per=0):
		if self.mode == 'kernel_only':
			w, b = layer.get_weights()
			#sum = np.sum(np.linalg.norm(w))
			score = self.scores[key]
			value_per = np.percentile(np.abs(score), q_per)
			index_cut = np.where(value_per <= np.abs(score))
			print(w.shape, score.shape, value_per)
			w[:,:,:,index_cut] = 0
			#sum1 = np.sum(np.linalg.norm(w))
			#w = w*(sum/sum1)
			layer.set_weights([w, b])

	def start_pruning(self, nb_gen, pop_size, acc_drop, **args):
		# Init ALGO
		self.init_pruning()
		self.accs = []
		self.acc_drop = acc_drop
		self.weights = list(set([str(self.convert(w.name)) for w in self.model.trainable_weights]))
		self.weights_mass = [np.product(w.shape.as_list()) for i, w in enumerate(self.model.trainable_weights) if i%2 == 0]
		self.weights_sum = np.sum(self.weights_mass)
		self.L_weights = len(self.weights)
		self.sum_params = np.sum(self.weights_mass)
		self.accs.append(self.evaluate(self.model, False))
		self.NGEN = nb_gen # number of generations
		self.CXPB = 0.6
		self.MUTPB = 0.1

		creator.create('FitnessMax', base.Fitness, weights=(1.0,))
		creator.create("Individual", list, fitness=creator.FitnessMax)

		toolbox = base.Toolbox()
		toolbox.register("attr_float", np.random.uniform, 0.75, 1)
		toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_float, n=self.L_weights )
		toolbox.register("population", tools.initRepeat, list, toolbox.individual)
		toolbox.register("mate", tools.cxUniform, indpb=0.1)
		toolbox.register("mutate",  tools.mutGaussian, mu=0, sigma=0.1, indpb=0.1)
		toolbox.register("select", tools.selTournament, tournsize=3)

		self.toolbox = toolbox
		self.pop = toolbox.population(n=self.NGEN)

		self.init_loop()
		print(tools.selBest(self.pop, k=2))
		for g in range(self.NGEN):
			print(g+1)
			self.loop()
			print(tools.selBest(self.pop, k=2))

	def create_eval_model(self):
		self.eval_model = self.clone_keras_model()

	def clone_keras_model(self):
		eval_model = clone_model(self.model)
		for l, l2 in zip(self.model.layers, eval_model.layers):
			try:
				l2.set_weights(l.get_weights())
			except:
				pass
		return eval_model

	def eval_function(self, ind, index_ind):
		def score_mass(ind):
			return np.sum(np.multiply(ind, self.weights_mass))/self.sum_params

		def score_ind(acc_score, score_mass):
			acc_eval = (self.acc_drop - (self.current_acc - acc_score))/(self.acc_drop)
			mass_eval = np.log(score_mass)

			return acc_eval*mass_eval
		gc.collect()
		self.eval_model = self.clone_keras_model()
		for w_name, value in zip(self.weights, ind):
			if not 'dense' in w_name:
				layer = self.get_layer(self.eval_model, w_name)
				self.prune_layer_per_percentage(layer, w_name, q_per=value)
		acc_score = self.evaluate(self.eval_model, True ,is_returned=True)
		scored_mass = score_mass(ind)
		score = score_ind(acc_score, scored_mass)
		print(index_ind, acc_score, scored_mass, score, ind)
		print()
		gc.collect()
		return [float(score)]

	def load_weights(self, name):
		self.model.load_weights(name)

	def init_loop(self):
		fitnesses = []
		for index_ind, ind in enumerate(self.pop):
			fitnesses.append(self.eval_function(ind,index_ind))
		for ind, fit in zip(self.pop, fitnesses):
			ind.fitness.values = fit

	def loop(self):
		pop = self.toolbox.select(self.pop, k=len(self.pop))
		pop = [self.toolbox.clone(ind) for ind in pop]

		for child1, child2 in zip(pop[::2], pop[1::2]):
			if random.random() < self.CXPB:
				self.toolbox.mate(child1, child2)
				del child1.fitness.values, child2.fitness.values

		for mutant in pop:
			if random.random() < self.MUTPB:
				self.toolbox.mutate(mutant)
				del mutant.fitness.values

		valids = [ind for ind in pop if ind.fitness.valid]
		fitnesses = []
		for index_ind, ind in enumerate(valids):
			fitnesses.append(self.eval_function(ind,index_ind))
		for ind, fit in zip(valids, fitnesses):
			ind.fitness.values = fit

		self.accs.append(self.evaluate(self.model, False))

class OnlineMean:

	def __init__(self, key, nb_class):
		self.key = key
		self.nb_class = nb_class
		self.cnt = np.zeros((self.nb_class,))
		self.arr = [np.array(0) for _ in range(self.nb_class)]
	@property
	def key(self):
		return self.key

	@property
	def cnt(self):
		return self.cnt

	@property
	def arr(self):
		return self.arr

	def add_arr(self, ob, index):
		index = int(index)
		if self.cnt[index] == 0:
			self.arr[index] = ob
		else:
			self.arr[index] += ob#(1/float(self.cnt[index]+1)) * ((self.cnt[index])*self.arr[index] + ob)

		self.cnt[index]+=1

	def get_arr_by_index(self, index):
		index = int(index)
		return self.arr[index]

	def rescale(self, v):
		v = np.abs(v)
		v /= np.linalg.norm(v*np.max(v))
		return v

	def get_scores(self):
		return np.array([self.rescale(np.mean(self.arr, axis=0))])

def get_layer_rec_old(model, layer_name):
	print(model.layers[-1].name)
	for layer in model.layers:
		print (layer.name)
		if isinstance(layer, Model):
			return get_layer_rec(layer, layer_name)
		else:
			print('hello')
			if layer.name == layer_name:
				return layer



class NVIDIA_Aggregator:

	def __init__(self, model, name, optim, nb_class=None):
		self.model = model
		self.name = name
		self.optim = optim
		self.path = os.path.join(ROOT_PRUNED, name)
		if isinstance(nb_class, types.NoneType):
			raise Exception
		self.nb_class = nb_class
		self.reset_path()
		self.create_holder()

	def filter_tensor_name(self, w_name):
		return str('_'.join(w_name.split('/')[0].split('_')[:2]))

	def get_weight_names(self):
		w_names = []
		self.w_names = sorted(set([self.filter_tensor_name(w.name) for w in self.model.trainable_weights]))
		for index, w_name in enumerate(self.w_names):
			path2save = join_path([ROOT_PRUNED, self.name, 'scores', w_name, '*', self.optim + '.npy'])
			paths = glob.glob(path2save)
			print(path2save, paths)
			if len(paths) == 0:
				w_names.append(w_name)
		self.w_names = w_names


	def get_paths(self):
		self.get_weight_names()
		return [glob.glob(join_path([ROOT_PRUNED, self.name, self.filter_tensor_name(w_name), '*', self.optim, '*'])) for w_name in self.w_names]

	def reset_path(self):
		self.paths = self.get_paths()

	def create_holder(self):
		self.h = {}
		for w_name in self.w_names:
			self.h[w_name] = OnlineMean(w_name, self.nb_class)

	def check_paths(self):
		self.paths = self.get_paths()
		assert len(self.paths) == len(self.w_names), 'Wierd'
		p = ([len(f) for f in self.paths])
		print(p)
		assert np.sum(p) == len(p)*p[0], 'Wierd'

	def create_scores(self):
		self.d = defaultdict(int)
		paths = self.get_paths()
		self.scores = []
		root_path_L = len(ROOT_PRUNED.split('/'))
		for index, paths_weights in enumerate(paths):
			print(index)
			for path_weight in paths_weights:
				w_name, _class, _, _ = path_weight.split('/')[root_path_L + 1:]
				_class = int(_class)
				ob = -1*np.load(path_weight)
				self.h[w_name].add_arr(ob, _class)

		for w_name in self.h.keys():
			for _class in range(self.nb_class):
				path2dir = join_path([ROOT_PRUNED, self.name, 'scores', w_name])
				path2dir = os.path.join(path2dir, str(_class))
				mk(path2dir)
				path2save = os.path.join(path2dir, self.optim+'.npy')
				ob, cnt = self.h[w_name].get_arr_by_index(_class), self.h[w_name].cnt[_class]
				print(w_name, _class, ob.shape, cnt)
				arr2save = [ob, cnt]
				np.save(path2save, arr2save)

class NVIDIA_Extractor:

	def __init__(self, model, name, model_type):
		self.model = model
		self.name = name
		self.model_type = model_type
		self.path = os.path.join(ROOT_PRUNED, name)
		if not isinstance(self.path, types.StringType):
			raise InputError('[INFO] path has to be defined')
		else:
			if os.path.isdir(self.path):
				pass
			else:
				print(ROOT_PRUNED)
				if os.path.isdir (ROOT_PRUNED):
					print('[INFO] ROOT_PRUNED exists')
					mk (self.path)
				else:
					raise InputError ('[INFO] ROOT_PRUNED has to be a path to a directory')
		self.get_optim()

	def get_layer(self, model, layer_name):
		return get_layer_rec(model, layer_name)

	def check_optim(self):
		if self.optim.lower() not in self.optims:
			raise InputError(str(self.optim.lower())+' not in '+str(self.optims))

	def set_generators(self, train_pipe, test_pipe):
		self.train_pipe = train_pipe
		self.n = self.train_pipe.iter.n
		self.indexes_holder = np.zeros((self.n))
		self.test_pipe = test_pipe
		self.gen_func = generator

	def create_optim(self, **args):
		if self.optim == 'yellowfin':

			filtered_args = filter_args (YFOptimizer, args)
			print(filtered_args)
			opt = TFOptimizer(YFOptimizer(**filtered_args))

		elif self.optim == 'ftml':
			filtered_args = filter_args (FTML, args)
			opt = FTML(**filtered_args)
		else:
			for name, optim_class in inspect.getmembers (optimizers):
				if inspect.isclass(optim_class) and name[0] == name[0].upper():
					filtered_args = filter_args(optim_class, args)
					return  optim_class(**filtered_args)
		return opt

	def compile(self, optim, **args):
		self.optim = optim
		self.check_optim()
		opt = self.create_optim(**args)
		self.model.compile(opt, loss='categorical_crossentropy', metrics=['accuracy'])

	def filter_tensor_name(self, w_name):
		return str('_'.join(w_name.name.split('/')[0].split('_')[:-1]))

	def define_gradients(self):
		"""Return the gradient of every trainable weight in model

		Parameters
		-----------
		model : a keras model instance

		First, find all tensors which are trainable in the model. Surprisingly,
		`model.trainable_weights` will return tensors for which
		trainable=False has been set on their layer (last time I checked), hence the extra check.
		Next, get the gradients of the loss with respect to the weights.

		"""
		#weights = [tensor for index, tensor in enumerate(self.model.trainable_weights)]
		#names = [tensor.name for index, tensor in enumerate(self.model.trainable_weights)]
		#optimizer = self.model.optimizer
		#grads = optimizer.get_gradients (self.model.total_loss, weights)
		#self.tensors = [[n.replace('/','_') for n in names], weights, grads]
		acts = []
		names = []
		for index, w in enumerate(self.model.trainable_weights):
			name = w.name
			if index % 2 == 0:
				name = '_'.join(name.split('_')[:2])
				names.append(name)
				layer = self.get_layer(self.model, name)
				acts.append(layer.output)
		grads = self.model.optimizer.get_gradients(loss=self.model.total_loss, params=acts)
		self.tensors = [names, acts, grads]
		assert len(names) + len(acts) + len(grads) == 3*len(names), 'Wierd'

	def define_properties(self):
		self.inp = self.model.input


	def save_scores(self, values, name, _class):
		name = name.replace('.png', '.npy')
		for index in range(len(values[0])):
			tensor_name = values[0][index][1]
			act = values[0][index][0]
			grad = values[1][index][0]
			prod = np.multiply(act, grad)
			if 'conv' in tensor_name:
				prod = np.sum(np.sum(np.sum(prod, axis=0), axis=0), axis=0)
			path_prod_path = join_path([self.path, tensor_name, _class, self.optim])
			mk(path_prod_path)
			path_prod_name = os.path.join(path_prod_path, name)
			np.save(path_prod_name, prod)

	def define_functions(self, tensors_grad, tensors_weights):
		inputs = [self.model.inputs[0],  # input data
						 self.model.sample_weights[0],
						 self.model.targets[0],  # labels
						 K.learning_phase (),  # train or test mode
						 ]
		self.get_gradients = K.function(inputs=inputs, outputs=tensors_grad.tolist())
		self.get_acts = K.function(inputs=inputs, outputs=tensors_weights.tolist())

	def evaluate_path(self, tensor_name, _class, name):
		name = name.replace('.png','.npy')
		path_act_name = join_path([self.path, tensor_name, _class, self.optim, name])
		path_grad_name = join_path([self.path, tensor_name, _class, self.optim, name])
		is_act = False
		is_grad = False
		if os.path.exists(path_act_name):is_act = True
		if os.path.exists(path_grad_name): is_grad = True
		return is_act, is_grad


	def _extract(self, x, y, z):
		inputs = [[x],  # X
				  [1],
				  [y],  # y
				  1  # learning phase in TRAIN mode
				  ]
		name = self.get_file_name(z)
		_class = str(np.argmax(y, axis=-1))
		acts_tensors = []
		grads_tensors = []
		for index_tensor, tensor_name in enumerate(self.tensors[0]):
			is_act, is_grad = self.evaluate_path(tensor_name, _class, name)
			if not is_act:
				acts_tensors.append([tensor_name, self.tensors[1][index_tensor]])
			if not is_grad:
				grads_tensors.append([tensor_name, self.tensors[2][index_tensor]])

		acts_tensors = np.array(acts_tensors)
		grads_tensors = np.array(grads_tensors)
		self.define_functions(grads_tensors[:,1], acts_tensors[:,1])
		acts = [(x,y) for x,y in zip(self.get_acts(inputs), acts_tensors[:,0])]
		grads = [(x,y) for x,y in zip(self.get_gradients(inputs), grads_tensors[:,0])]

		self.save_scores((acts, grads), name, _class)

	def get_file_name(self, z):
		return self.train_pipe.augmentor_images[z].image_file_name

	def get_output_path(self, y, z):
		name = self.get_file_name(z)
		_class = str(np.argmax(y))
		return join_path([ROOT_PRUNED, _class, name])

	def extract_tensors(self):
		cnt = 0
		self.define_gradients()
		self.define_properties()
		self.gen_train = self.gen_func(self.train_pipe, self.model_type, with_indexes=True)
		for X, Y, Z in self.gen_train:
			if cnt == self.n:
				break
			for x, y, z in zip(X, Y, Z):
				if self.indexes_holder[z] == 0.:
					self.indexes_holder[z] = 1
					self._extract(x, y, z)
					cnt += 1
			print(cnt)

	def get_optim(self, extras=['yellowFin', 'ftml']):
		self.optims = []
		for name, class_obj in inspect.getmembers(optimizers):
			c_name = str(class_obj)
			if 'class' in c_name:
				if  name.lower() not in ['optimizer', 'tfoptimizer','__builtins__', '__doc__']:
					self.optims.append(name.lower())
		self.optims = list(set(self.optims))
		self.optims+=[op.lower() for op in extras]